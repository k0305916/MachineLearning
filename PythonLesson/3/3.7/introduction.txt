keywords:
此类模型的特点是会对训练数据进行记忆；
而惰性学习（lazy learning）则是基于实例学习的一个特例，它在学习阶段的计算成本为0。

KNN算法本身是很简单的，可以归纳为以下几步：
1）选择近邻的数量k和距离度量方法。
2）找到待分类样本的k个最近邻居。
3）根据最近邻的类标进行多数投票。

就KNN来说，找到正确的k值是在过拟合与欠拟合之间找到平衡的关键所在。

“闵可夫斯基”（'minkowski'）距离是对欧几里得距离及曼哈顿距离的一种泛化，可写作：

如果将参数设定为p＝2，则为欧几里得距离；当p＝1时，就是曼哈顿距离。

scikit-learn中还实现了许多其他的距离度量标准，
具体内容可见如下网址：
http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html。

由于维度灾难（curse of dimensionality）的原因，使得KNN算法易于过拟合。
维度灾难是指这样一种现象：对于一个样本数量大小稳定的训练数据集，
随着其特征数量的增加，样本中有具体值的特征数量变得极其稀疏（大多数特征的取值为空）。

在逻辑斯谛回归一节中，我们讨论了通过正则化使其避免过拟合。
不过，正则化方法并不适用于诸如决策树和KNN等算法，
但可以使用特征选择和降维等技术来帮助其避免维度灾难。