keywords:
# 降维技术主要分为两个大类：特征选择和特征提取。通过特征选择，我们可以选出原始特征的一个子集。
# 而在特征提取中，通过对现有的特征信息进行推演，构造出一个新的特征子空间。

# 序列特征选择算法系一种贪婪搜索算法，用于将原始的d维特征空间压缩到一个k维特征子空间，其中k<d
# 使用特征选择算法出于以下考虑：能够剔除不相关特征或噪声，
# 自动选出与问题最相关的特征子集，从而提高计算效率或是降低模型的泛化误差。
# 一个经典的序列特征选择算法是序列后向选择算法（Sequential Backward Selection，SBS），
# 其目的是在分类性能衰减最小的约束下，降低原始特征空间上的数据维度，以提高计算效率。
# 在某些情况下，SBS甚至可以在模型面临过拟合问题时提高模型的预测能力。

# SBS算法背后的理念非常简单：SBS依次从特征集合中删除某些特征，直到新的特征子空间包含指定数量的特征。

为了确定每一步中所需删除的特征，我们定义一个需要最小化的标准衡量函数J。
该函数的计算准则是：比较判定分类器的性能在删除某个特定特征前后的差异,
即每一步中特征被删除后，所引起的模型性能损失最小。

我们可以将算法总结为四个简单的步骤：
1）设k＝d进行算法初始化，其中d是特征空间Xd的维度。
2）定义x-为满足标准x-＝argmaxJ（Xk-x）最大化的特征，其中x∈Xk。
3）将特征x-从特征集中删除：Xk-1＝Xk-x-，k＝k-1。
4）如果k等于目标特征数量，算法终止，否则跳转到第2步。


scikti-learn提供了许多其他的特征选择算法，包括：
1、基于特征权重的递归后向消除算法（recursive backward elimination based on feature weights）
2、基于特征重要性的特征选择树方法（tree-based methods to select features by importance）
3、单变量统计测试（univariate statistical tests）
对特征选择方法进行更深入的讨论已经超出了本书的范围，对这些算法的总结性描述及其示例请参见链接：
http://scikit-learn.org/stable/modules/feature_selection.html。