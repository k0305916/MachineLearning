keywords:

本章我们将进入监督学习的另一个分支：回归分析（regression analysis）。

基于前面所定义的线性方程，线性回归可看作是求解样本点的最佳拟合直线.
这条最佳拟合线也被称为回归线（regression line），回归线与样本点之间的垂直连线即所谓的偏移（offset）或残差（residual）——预测的误差。


请注意，不同于人们通常的理解，训练一个线性回归模型并不需要解释数量或者目标变量呈正态分布。正态假设仅适用于某些统计检验和假设检验

为了拟合线性回归模型，我们主要关注那些跟目标变量MEDV高度相关的特征。


梯度下降算法能够进行收敛性检查，使用这类优化算法时，将代价看作迭代次数的函数（基于训练数据集），并将其绘制成图是个非常好的做法。

在大多数介绍统计科学的教科书中，都可以找到使用最小二乘法求解线性方程组的封闭方法：expression.png

其中，μy为真实目标值的均值，为经预测得到目标值的均值。
这种方法的优点在于：它一定能够分析找到最优解。不过，如果要处理的数据集量很大，公式中逆矩阵的计算成本会非常高，或者矩阵本身为奇异矩阵（不可逆），这就是在特定情况下我们更倾向于使用交互式方法的原因。
若读者对如何得到正规方程感兴趣，建议阅读Stephen Pollock博士在英国莱斯特大学演讲讲义中的章节“The Classical Linear Regression Model”，可通过链接http://www.le.ac.uk/users/dsgp1/COURSES/MESOMET/ECMETXT/06mesmet.pdf获取。


正则化是通过在模型中加入额外信息来解决过拟合问题的一种方法，引入罚项增加了模型的复杂性但却降低了模型参数的影响。最常见的正则化线性回归方法就是所谓的岭回归（Ridge Regression）、最小绝对收缩及算子选择（Least Absolute Shrinkage and Selection Operator，LASSO）以及弹性网络（Elastic Net）等。